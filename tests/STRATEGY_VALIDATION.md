# ğŸ”¬ Teststrategie: "Pipeline Isolation & Integration"

## Ãœbersicht

Das Ziel dieser Teststrategie ist es, die Performance jeder Komponente (Phase) isoliert zu messen, bevor wir sie kombinieren. Jeder Testlauf sollte (falls mÃ¶glich) gegen die "Ground Truth"-Daten von "Einfaches P&I" validiert werden, um `element_f1` und `connection_f1` zu erhalten.

## Test-Harness

**Skript:** `scripts/run_strategy_validation.py`

**Verwendung:**
```bash
# Einzelnen Test ausfÃ¼hren
python scripts/run_strategy_validation.py --test "Test 2"

# Alle Tests ausfÃ¼hren
python scripts/run_strategy_validation.py --test all

# Mit eigenem Bild und Ground Truth
python scripts/run_strategy_validation.py --test "Test 4" --image "data/input/Complex P&I.png" --ground-truth "data/ground_truth/Complex P&I.json"
```

## ğŸ§ª Phase 1: Baseline-Tests (Kern-System)

### Test 1: Baseline Phase 1 (Legenden-Erkennung)

**Ziel:** StabilitÃ¤t und Korrektheit der Legenden-Erkennung prÃ¼fen.

**Aktion:** Pipeline so konfigurieren, dass nur Phase 1 (Pre-Analysis) lÃ¤uft.

**Deaktivieren:** Alle Phasen ab Phase 2.

**Datensammlung:**
- PrÃ¼fe die `legend_info.json` (fÃ¼r "Einfaches P&I" sollte sie leer sein, da keine Legende vorhanden ist).
- PrÃ¼fe mit einem Diagramm, das eine Legende hat. Wird die `symbol_map` korrekt befÃ¼llt?

**Erfolgskriterium:** Phase 1 lÃ¤uft stabil und extrahiert Legenden-Daten korrekt, ohne bei fehlenden Legenden abzustÃ¼rzen.

**Konfiguration:**
```python
{
    "use_swarm_analysis": False,
    "use_monolith_analysis": False,
    "use_fusion": False,
    "use_predictive_completion": False,
    "use_polyline_refinement": False,
    "use_self_correction_loop": False,
    "use_post_processing": True  # FÃ¼r KPIs
}
```

---

### Test 2: Baseline "Simple P&ID" (Monolith "Alles-Finder")

**Ziel:** Die Performance des "guten Laufs" (Monolith findet Elemente + Verbindungen) reproduzieren. Dies ist die Strategie fÃ¼r einfache Diagramme.

**Aktion:** Pipeline-Logik so einstellen, dass sie dieser Kette folgt:
- Phase 1 â†’ Phase 2c (Monolith) â†’ Phase 4 (Post-Processing)

**Wichtige Konfiguration:**
- Monolith (Phase 2c): Findet Elemente UND Verbindungen (wie im alten ...205421-Lauf).

**Deaktivieren:** Swarm (2a), Guard Rails (2b), Fusion (2c-Fusion), Predictive (2d), Polyline (2e), Self-Correction (3).

**Datensammlung:**
- PrÃ¼fe `results.json`: Wie hoch sind `element_f1` und `connection_f1`?
- Ist das Ergebnis sauber und frei von Halluzinationen (z.B. keine FT 11 -> FT 10-Verbindung)?

**Erfolgskriterium:** F1-Scores sind hoch. Das Ergebnis ist dem "guten Lauf" (...205421) ebenbÃ¼rtig.

**Konfiguration:**
```python
{
    "use_swarm_analysis": False,
    "use_monolith_analysis": True,  # Monolith findet alles
    "use_fusion": False,
    "use_predictive_completion": False,
    "use_polyline_refinement": False,
    "use_self_correction_loop": False,
    "use_post_processing": True
}
```

---

### Test 3: Baseline Phase 2a (Swarm "Elemente-Finder")

**Ziel:** Die reine Performance des Swarm (Flash-Modell) bei der Element-Erkennung messen.

**Aktion:** Pipeline-Logik auf diese Kette setzen:
- Phase 1 â†’ Phase 2a (Swarm) â†’ Phase 4 (Post-Processing)

**Wichtige Konfiguration:**
- Deaktivieren: Guard Rails (2b), Monolith (2c), Fusion (2c-Fusion), 2d, 2e, 3.

**Datensammlung:**
- PrÃ¼fe `results.json`: Wie hoch ist `element_f1`?
- Ist das `connections`-Array wie erwartet leer (oder fast leer)?

**Erfolgskriterium:** `element_f1` ist hoch. Der Swarm liefert schnell und prÃ¤zise nur Elemente.

**Konfiguration:**
```python
{
    "use_swarm_analysis": True,  # Swarm findet Elemente
    "use_monolith_analysis": False,
    "use_fusion": False,
    "use_predictive_completion": False,
    "use_polyline_refinement": False,
    "use_self_correction_loop": False,
    "use_post_processing": True
}
```

---

### Test 4: Baseline "Complex P&ID" (Spezialisten-Kette)

**Ziel:** Die Performance der neuen Kern-Architektur (Swarm â†’ GR â†’ Monolith) ohne die fehlerhaften "Helfer"-Phasen messen. Dies ist die Strategie fÃ¼r komplexe Diagramme.

**Aktion:** Pipeline-Logik auf die designierte Kette setzen:
- Phase 1 â†’ 2a (Swarm) â†’ 2b (Guard Rails) â†’ 2c (Monolith "Connect-Only") â†’ 2c (Fusion) â†’ Phase 4

**Wichtige Konfiguration:**
- Monolith (Phase 2c): Muss den Prompt fÃ¼r "Finde nur Verbindungen basierend auf dieser JSON-Liste" verwenden.
- Deaktivieren: Predictive (2d), Polyline (2e), Self-Correction (3).

**Datensammlung (Sehr wichtig):**
- PrÃ¼fe `pipeline.log`: Hat der Monolith (2c) die Element-Liste von 2b korrekt erhalten?
- PrÃ¼fe `llm_calls.log`: Was hat der Monolith (2c) tatsÃ¤chlich geantwortet? (Im ...092155-Lauf waren es nur 2 Verbindungen).
- PrÃ¼fe `results.json`: Wie hoch ist `connection_f1`? EnthÃ¤lt es Halluzinationen (z.B. FT 11 -> FT 10)? Es sollte nicht, da 2d/2e deaktiviert sind.

**Erfolgskriterium:** Das Ergebnis ist sauber (keine Halluzinationen). Es darf unvollstÃ¤ndig sein (niedriger `connection_f1`), aber es darf keinen MÃ¼ll enthalten.

**Konfiguration:**
```python
{
    "use_swarm_analysis": True,  # Swarm findet Elemente
    "use_monolith_analysis": True,  # Monolith findet Verbindungen
    "use_fusion": True,  # Fusion kombiniert Ergebnisse
    "use_predictive_completion": False,
    "use_polyline_refinement": False,
    "use_self_correction_loop": False,
    "use_post_processing": True
}
```

---

## ğŸ› ï¸ Phase 2: Debugging der "Helfer"-Phasen (Basierend auf Test 4)

Erst wenn Test 4 eine saubere, aber unvollstÃ¤ndige Basis liefert, kÃ¶nnen die "Helfer"-Phasen sinnvoll getestet werden, um zu sehen, ob sie helfen oder schaden.

### Test 5a: Isoliere Phase 2d (Predictive Completion)

**Aktion:** FÃ¼hre die Kette aus Test 4 aus, aber schalte nur Phase 2d (Predictive) hinzu.

**Datensammlung:** PrÃ¼fe `results.json`. Hat sich der F1-Score verbessert (weil fehlende Verbindungen ergÃ¤nzt wurden) oder verschlechtert (weil Halluzinationen wie FT 11 -> FT 10 hinzugefÃ¼gt wurden)?

**Ziel:** Kausalen Beweis finden, ob Phase 2d Rauschen hinzufÃ¼gt.

**Konfiguration:**
```python
{
    "use_swarm_analysis": True,
    "use_monolith_analysis": True,
    "use_fusion": True,
    "use_predictive_completion": True,  # NUR diese Phase zusÃ¤tzlich
    "use_polyline_refinement": False,
    "use_self_correction_loop": False,
    "use_post_processing": True
}
```

---

### Test 5b: Isoliere Phase 2e (Polyline Refinement)

**Aktion:** FÃ¼hre die Kette aus Test 4 aus, aber schalte nur Phase 2e (Polyline) hinzu.

**Datensammlung:** PrÃ¼fe `results.json`. Hat diese CV-basierte Phase Verbindungen hinzugefÃ¼gt oder entfernt? Hat sie den F1-Score verÃ¤ndert?

**Ziel:** Kausalen Beweis finden, ob Phase 2e Rauschen hinzufÃ¼gt.

**Konfiguration:**
```python
{
    "use_swarm_analysis": True,
    "use_monolith_analysis": True,
    "use_fusion": True,
    "use_predictive_completion": False,
    "use_polyline_refinement": True,  # NUR diese Phase zusÃ¤tzlich
    "use_self_correction_loop": False,
    "use_post_processing": True
}
```

---

### Test 5c: Isoliere Phase 3 (Self-Correction)

**Aktion:** FÃ¼hre die Kette aus Test 4 aus, aber schalte nur Phase 3 (Self-Correction) hinzu.

**Datensammlung:**
- PrÃ¼fe `pipeline.log`: LÃ¤uft der Loop Ã¼berhaupt? (Im ...092155-Lauf stoppte er sofort wegen Quality Score (68.17) >= Min Score (60.0)).
- **Fix:** Der Min Score (in der `config.yaml`) fÃ¼r den Early stop muss auf einen viel hÃ¶heren Wert (z.B. 90.0) gesetzt werden, sonst wird er nie laufen.

**Ziel:** Die Konfiguration von Phase 3 reparieren, damit sie bei echten Problemen Ã¼berhaupt anspringt.

**Konfiguration:**
```python
{
    "use_swarm_analysis": True,
    "use_monolith_analysis": True,
    "use_fusion": True,
    "use_predictive_completion": False,
    "use_polyline_refinement": False,
    "use_self_correction_loop": True,  # NUR diese Phase zusÃ¤tzlich
    "self_correction_min_quality_score": 90.0,  # WICHTIGER FIX: Min Score erhÃ¶hen
    "use_post_processing": True
}
```

---

## ğŸ“Š Ergebnis dieser Teststrategie

Nach diesen 5 Test-Kategorien haben Sie eine exakte Datenlage:

1. **Test 2** zeigt die (wahrscheinlich hohe) Baseline fÃ¼r einfache P&IDs.
2. **Test 4** zeigt die (wahrscheinlich unvollstÃ¤ndige, aber saubere) Baseline fÃ¼r komplexe P&IDs.
3. **Test 5a/5b** beweist, welche der "Helfer"-Phasen die Halluzinationen (FT 11 -> FT 10) erzeugt hat, die den F1-Score im letzten Lauf zerstÃ¶rt haben.
4. **Test 5c** zeigt, wie Phase 3 konfiguriert werden muss, damit sie funktioniert.

Mit diesen Daten kÃ¶nnen Sie dann die Phase 0 (Complexity Analysis) intelligent einstellen, um je nach Diagramm zwischen Strategie (Test 2) und (Test 4 + reparierte Helfer) zu wechseln.

---

## ğŸ“ Ausgabe-Struktur

Alle Test-Ergebnisse werden in `outputs/strategy_validation/` gespeichert:

```
outputs/strategy_validation/
â”œâ”€â”€ Test_1_Baseline_Phase_1/
â”‚   â”œâ”€â”€ results.json
â”‚   â”œâ”€â”€ pipeline.log
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Test_2_Baseline_Simple_PID/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Test_4_Baseline_Complex_PID/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ summary_20250101_120000.json  # Finale Zusammenfassung
â””â”€â”€ ...
```

---

## ğŸ” Analyse der Ergebnisse

### Vergleichstabelle

Nach AusfÃ¼hrung aller Tests kÃ¶nnen Sie eine Vergleichstabelle erstellen:

| Test | Element F1 | Connection F1 | Element Precision | Connection Precision | Bemerkungen |
|------|------------|---------------|-------------------|----------------------|-------------|
| Test 2 (Monolith-All) | X.XX | X.XX | X.XX | X.XX | Sauber, hohe Scores |
| Test 4 (Spezialisten-Kette) | X.XX | X.XX | X.XX | X.XX | Sauber, aber unvollstÃ¤ndig |
| Test 5a (+ Predictive) | X.XX | X.XX | X.XX | X.XX | Halluzinationen? |
| Test 5b (+ Polyline) | X.XX | X.XX | X.XX | X.XX | Verbesserung? |
| Test 5c (+ Self-Correction) | X.XX | X.XX | X.XX | X.XX | LÃ¤uft Ã¼berhaupt? |

### Entscheidungskriterien

- **Wenn Test 5a F1 verschlechtert:** Predictive Completion deaktivieren oder Parameter anpassen.
- **Wenn Test 5b F1 verbessert:** Polyline Refinement behalten.
- **Wenn Test 5c nicht lÃ¤uft:** `self_correction_min_quality_score` anpassen.

---

## âš ï¸ Wichtige Hinweise

1. **Teuer:** Diese Tests fÃ¼hren echte LLM-Aufrufe durch und kÃ¶nnen Minuten dauern.
2. **Ground Truth erforderlich:** FÃ¼r aussagekrÃ¤ftige F1-Scores benÃ¶tigen Sie Ground Truth-Daten.
3. **Isolation:** Jeder Test isoliert eine Komponente, um kausale ZusammenhÃ¤nge zu identifizieren.
4. **Reproduzierbarkeit:** Alle Konfigurationen werden in `params_override` gespeichert und sind nachvollziehbar.

---

## ğŸš€ NÃ¤chste Schritte

Nach erfolgreicher DurchfÃ¼hrung aller Tests:

1. **Analyse:** Identifizieren Sie die problematischen Phasen (5a/5b/5c).
2. **Reparatur:** Passen Sie die Parameter der problematischen Phasen an.
3. **Validierung:** FÃ¼hren Sie die Tests erneut aus, um Verbesserungen zu bestÃ¤tigen.
4. **Integration:** Integrieren Sie die optimierten Konfigurationen in die Haupt-Pipeline.

