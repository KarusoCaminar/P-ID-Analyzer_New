# ğŸ§ª Test Overview - Alle verfÃ¼gbaren Tests

**Datum:** 2025-11-07  
**Status:** Ãœbersicht aller verfÃ¼gbaren Test-Skripte und -Strategien

---

## ğŸ“‹ Test-Kategorien

### **1. Live Test Scripts (Empfohlen fÃ¼r einzelne TestlÃ¤ufe)**

#### **`run_live_test.py`** â­ **HAUPT-TEST-SKRIPT**
- **Zweck:** FÃ¼hrt einen einzelnen Testlauf mit Live-Log-Monitoring durch
- **Features:**
  - Live-Log-Monitoring im Terminal
  - Strukturierte Ausgabe (OutputStructureManager)
  - UnterstÃ¼tzt verschiedene Bilder: `simple`, `complex`, `uni`
  - Strategie-Auswahl: `--strategy` (z.B. `hybrid_fusion`, `simple_whole_image`)
  - KPI-Berechnung mit Ground Truth
  - Speichert alle Ergebnisse in `outputs/live_test/`
- **Verwendung:**
  ```bash
  python scripts/validation/run_live_test.py --image uni --strategy hybrid_fusion
  ```
- **Output:** `outputs/live_test/{timestamp}/`

#### **`run_simple_test.py`**
- **Zweck:** Schneller Testlauf fÃ¼r einfache Validierung
- **Features:**
  - Minimaler Testlauf
  - Keine Live-Logs
  - Schnellere AusfÃ¼hrung
- **Verwendung:**
  ```bash
  python scripts/validation/run_simple_test.py
  ```

---

### **2. Parameter Tuning Tests**

#### **`run_parameter_tuning.py`** âš™ï¸ **PARAMETER-OPTIMIERUNG**
- **Zweck:** Optimiert `adaptive_threshold_factor`, `adaptive_threshold_min`, `adaptive_threshold_max`
- **Features:**
  - Testet verschiedene Parameter-Kombinationen
  - Berechnet Connection F1-Score fÃ¼r jede Kombination
  - Worker Pool fÃ¼r Parallelisierung
  - Live-Log-Monitoring
  - Speichert Ergebnisse in `outputs/parameter_tuning/`
- **Verwendung:**
  ```bash
  python scripts/validation/run_parameter_tuning.py
  ```
- **Output:** `outputs/parameter_tuning/{timestamp}/`

#### **`monitor_parameter_tuning.py`**
- **Zweck:** Ãœberwacht den Parameter-Tuning-Prozess
- **Features:**
  - Zeigt Fortschritt an
  - Beste Parameter-Kombination
  - Aktueller Status
- **Verwendung:**
  ```bash
  python scripts/validation/monitor_parameter_tuning.py
  ```

#### **`show_parameter_tuning_status.py`**
- **Zweck:** Zeigt Status nach Abschluss des Parameter-Tuning
- **Features:**
  - Statistik
  - Beste Parameter
  - KPIs
  - Warnungen (z.B. Connection F1 = 0.0)

---

### **3. Strategy Validation Tests**

#### **`run_strategy_validation.py`**
- **Zweck:** Testet verschiedene Strategien (z.B. `hybrid_fusion`, `simple_whole_image`)
- **Features:**
  - Vergleicht verschiedene Strategien
  - Berechnet KPIs fÃ¼r jede Strategie
  - Speichert Ergebnisse fÃ¼r Vergleich
- **Verwendung:**
  ```bash
  python scripts/validation/run_strategy_validation.py
  ```

#### **`run_strategy_validation_with_logs.py`**
- **Zweck:** Wie `run_strategy_validation.py`, aber mit detaillierten Logs
- **Features:**
  - Detaillierte Logs
  - Live-Monitoring
  - Bessere Fehlerdiagnose

---

### **4. Overnight Optimization Tests**

#### **`run_overnight_optimization.py`** ğŸŒ™ **ÃœBERNACHT-OPTIMIERUNG**
- **Zweck:** A/B-Testing und Optimierung Ã¼ber Nacht
- **Features:**
  - Testet verschiedene Strategien
  - Parameter-Optimierung
  - KPI-Berechnung
  - Active Learning
  - Automatische Wiederholung bei Fehlern
- **Verwendung:**
  ```bash
  python scripts/validation/run_overnight_optimization.py
  ```

#### **`monitor_overnight.py`**
- **Zweck:** Ãœberwacht den Overnight-Optimization-Prozess
- **Features:**
  - Fortschrittsanzeige
  - Log-Ãœberwachung
  - Fehler-Erkennung

#### **`watchdog_overnight.py`**
- **Zweck:** Watchdog fÃ¼r Overnight-Optimization
- **Features:**
  - Startet Prozess neu bei Absturz
  - Ãœberwacht Logs
  - Automatische Fehlerbehandlung

---

### **5. API Rate Limit Tests**

#### **`test_api_rate_limit.py`** ğŸš€ **RATE-LIMIT-TEST**
- **Zweck:** Testet API Rate Limits fÃ¼r verschiedene Modelle/Regionen
- **Features:**
  - Testet Flash und Pro Modelle
  - Verschiedene Worker-Anzahlen (15, 20, 25, 30, 40, 50)
  - Verschiedene Regionen (us-central1, europe-west3)
  - Berechnet maximale RPM
  - DSQ-Optimierung
- **Verwendung:**
  ```bash
  python scripts/validation/test_api_rate_limit.py
  ```
- **Output:** `outputs/rate_limit_test/rate_limit_test_results_{timestamp}.json`

#### **`analyze_rate_limit_results.py`**
- **Zweck:** Analysiert Rate-Limit-Test-Ergebnisse
- **Features:**
  - Empfehlungen fÃ¼r Config
  - Beste Worker-Anzahl
  - Beste Region
  - Maximale RPM

---

### **6. Performance Tests**

#### **`test_startup_speed.py`**
- **Zweck:** Misst Startup-Zeit verschiedener Komponenten
- **Features:**
  - KnowledgeManager Startup-Zeit
  - LLMClient Startup-Zeit
  - PipelineCoordinator Startup-Zeit
  - Gesamt-Startup-Zeit

---

### **7. Unit Tests**

#### **`tests/unit/test_*.py`**
- **Zweck:** Unit Tests fÃ¼r einzelne Komponenten
- **VerfÃ¼gbare Tests:**
  - `test_pipeline_coordinator.py` - Pipeline Coordinator Tests
  - `test_swarm_analyzer.py` - Swarm Analyzer Tests
  - `test_monolith_analyzer.py` - Monolith Analyzer Tests
  - `test_fusion_engine.py` - Fusion Engine Tests
  - `test_line_extractor.py` - Line Extractor Tests
  - `test_kpi_calculator.py` - KPI Calculator Tests
  - `test_cgm_generator.py` - CGM Generator Tests
  - `test_complexity_analyzer.py` - Complexity Analyzer Tests
- **Verwendung:**
  ```bash
  pytest tests/unit/
  ```

---

### **8. Test Harness Utilities**

#### **`src/utils/test_harness.py`** ğŸ”§ **TEST-HARNESS-UTILITIES**
- **Zweck:** Utilities fÃ¼r Test-Harness (Zwischenergebnisse, Config-Snapshots)
- **Features:**
  - `save_intermediate_result()` - Speichert Zwischenergebnisse nach jeder Phase
  - `save_config_snapshot()` - Speichert Config-Snapshot
  - `save_test_metadata()` - Speichert Test-Metadaten
  - Verwendet OutputStructureManager fÃ¼r strukturierte Ausgabe
- **Verwendung:** Wird automatisch von PipelineCoordinator verwendet

---

## ğŸ¯ Empfohlene Test-Reihenfolge

### **1. Einfacher Testlauf (Schnell)**
```bash
python scripts/validation/run_live_test.py --image simple --strategy simple_whole_image
```
- **Zweck:** Schneller Test, um zu prÃ¼fen, ob alles funktioniert
- **Dauer:** ~3-5 Minuten

### **2. Komplexer Testlauf (VollstÃ¤ndig)**
```bash
python scripts/validation/run_live_test.py --image uni --strategy hybrid_fusion
```
- **Zweck:** VollstÃ¤ndiger Testlauf mit komplexem Bild
- **Dauer:** ~10-20 Minuten
- **Features:**
  - Live-Log-Monitoring
  - KPI-Berechnung
  - Strukturierte Ausgabe

### **3. Parameter Tuning (Optimierung)**
```bash
python scripts/validation/run_parameter_tuning.py
```
- **Zweck:** Optimiert `adaptive_threshold_factor`, `adaptive_threshold_min`, `adaptive_threshold_max`
- **Dauer:** ~1-2 Stunden (36 Kombinationen)
- **Monitoring:**
  ```bash
  python scripts/validation/monitor_parameter_tuning.py
  ```

### **4. Strategy Validation (Vergleich)**
```bash
python scripts/validation/run_strategy_validation.py
```
- **Zweck:** Vergleicht verschiedene Strategien
- **Dauer:** ~30-60 Minuten
- **Output:** Vergleich verschiedener Strategien

### **5. Overnight Optimization (Langzeit)**
```bash
python scripts/validation/run_overnight_optimization.py
```
- **Zweck:** Langzeit-Optimierung Ã¼ber Nacht
- **Dauer:** ~8-12 Stunden
- **Monitoring:**
  ```bash
  python scripts/validation/monitor_overnight.py
  ```

---

## ğŸ“Š Test-Output-Struktur

### **Live Test Output:**
```
outputs/live_test/{timestamp}/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ test_result.json
â”‚   â””â”€â”€ output_phase_*.json
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ config_snapshot.yaml
â”‚   â””â”€â”€ test_metadata.json
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ debug_map.png
â”‚   â”œâ”€â”€ score_curve.png
â”‚   â””â”€â”€ kpi_dashboard.png
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ test.log
â””â”€â”€ README.md
```

### **Parameter Tuning Output:**
```
outputs/parameter_tuning/{timestamp}/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ parameter_tuning_results.json
â”‚   â””â”€â”€ parameter_tuning_summary.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ parameter_tuning.log
â””â”€â”€ README.md
```

---

## ğŸ” Monitoring & Debugging

### **Live Log Monitoring:**
```bash
# Python Monitor
python scripts/validation/monitor_live_test.py

# PowerShell Monitor
scripts/validation/watch_live_test.ps1
```

### **Log Analysis:**
```bash
# Zeige letzten 100 Zeilen
tail -n 100 outputs/live_test/{timestamp}/logs/test.log

# Suche nach Fehlern
grep -i error outputs/live_test/{timestamp}/logs/test.log
```

---

## âš™ï¸ Test-Konfiguration

### **VerfÃ¼gbare Strategien:**
- `simple_whole_image` - Einfache P&IDs (Monolith-only)
- `hybrid_fusion` - Komplexe P&IDs (Swarm + Monolith + Fusion)
- `optimal_swarm_monolith` - Balanced (Standard)
- `quality_focused` - Maximale QualitÃ¤t
- `default_flash` - Schnellste Strategie (nur Swarm)

### **VerfÃ¼gbare Bilder:**
- `simple` - Einfaches P&ID (`Einfaches P&I.png`)
- `complex` - Komplexes P&ID (`page_1_original.png`)
- `uni` - Uni-Bild (`VerfahrensflieÃŸbild_Uni.png`)

---

## ğŸ¯ NÃ¤chste Schritte

1. **Starte einfachen Testlauf:**
   ```bash
   python scripts/validation/run_live_test.py --image simple --strategy simple_whole_image
   ```

2. **Starte komplexen Testlauf:**
   ```bash
   python scripts/validation/run_live_test.py --image uni --strategy hybrid_fusion
   ```

3. **Ãœberwache Live-Logs:**
   ```bash
   python scripts/validation/monitor_live_test.py
   ```

4. **Analysiere Ergebnisse:**
   - Check `outputs/live_test/{timestamp}/data/test_result.json`
   - Check `outputs/live_test/{timestamp}/visualizations/kpi_dashboard.png`

---

## ğŸ“ Wichtige Hinweise

### **Test Harness:**
- **Automatisch aktiviert:** Test Harness wird automatisch von `PipelineCoordinator` verwendet
- **Zwischenergebnisse:** Werden in `data/output_phase_*.json` gespeichert
- **Config-Snapshots:** Werden in `artifacts/config_snapshot.yaml` gespeichert

### **Optimierungen:**
- **Worker:** 30 (Flash-optimiert)
- **RPM:** 500 (Flash-optimiert)
- **Region:** us-central1 (2.5x schneller)

### **Erwartete Performance:**
- **Flash:** 500-530 RPM (stabil)
- **Pro:** 24 RPM (stabil bei 15 Workers)
- **Pipeline:** ~40-50% Zeitersparnis mit optimierten Einstellungen

