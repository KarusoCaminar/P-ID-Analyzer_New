# ğŸ§ª Test-Strategie: VollstÃ¤ndige Dokumentation

**Datum:** 2025-11-06  
**Status:** âœ… Konfiguriert und bereit

---

## ğŸ“‹ Ãœbersicht

Diese Dokumentation beschreibt die vollstÃ¤ndige Test-Strategie fÃ¼r die P&ID-Analyse-Pipeline. Alle Tests werden automatisch validiert, ausgefÃ¼hrt und die Ergebnisse werden in einer sauberen Ordnerstruktur unter `outputs/strategy_validation/` gespeichert.

---

## ğŸ¯ Test-Ziele

Die Test-Strategie dient dazu:

1. **Pipeline-Komponenten isoliert zu testen** - Jede Phase einzeln validieren
2. **Performance zu messen** - F1-Scores, Precision, Recall fÃ¼r Elemente und Verbindungen
3. **Fehlerquellen zu identifizieren** - Welche Komponente verursacht welche Fehler?
4. **Strategien zu vergleichen** - Monolith vs. Swarm vs. Fusion
5. **Verbesserungen zu validieren** - Predictive, Polyline, Self-Correction

---

## ğŸ“Š Test-Ãœbersicht

### Test 1: Baseline Phase 1 (Legenden-Erkennung)
- **Ziel:** Nur Phase 1 (Pre-Analysis) testen
- **Bild:** `training_data/complex_pids/page_1_original.png` (MIT Legende)
- **Ground Truth:** `training_data/complex_pids/page_1_original_truth_cgm.json`
- **Aktiviert:** Nur Phase 1
- **Deaktiviert:** Alle anderen Phasen
- **Erwartung:** Legende wird erkannt, keine Elemente/Verbindungen

### Test 2: Baseline Simple P&ID (Monolith-All)
- **Ziel:** Monolith-Analyse auf einfachem Bild testen
- **Bild:** `training_data/simple_pids/Einfaches P&I.png` (Simple P&ID)
- **Ground Truth:** `training_data/simple_pids/Einfaches P&I_truth.json`
- **Aktiviert:** Phase 2 (Monolith), Phase 4 (Post-Processing)
- **Deaktiviert:** Swarm, Fusion, Predictive, Polyline, Self-Correction
- **Erwartung:** Gute F1-Scores fÃ¼r einfaches Bild

### Test 3: Baseline Swarm-Only
- **Ziel:** Swarm-Analyse isoliert testen
- **Bild:** `training_data/simple_pids/Einfaches P&I.png` (Simple P&ID)
- **Ground Truth:** `training_data/simple_pids/Einfaches P&I_truth.json`
- **Aktiviert:** Phase 2 (Swarm), Phase 4 (Post-Processing)
- **Deaktiviert:** Monolith, Fusion, Predictive, Polyline, Self-Correction
- **Erwartung:** Swarm findet Elemente, aber keine Verbindungen

### Test 4: Baseline Complex P&ID (Spezialisten-Kette)
- **Ziel:** VollstÃ¤ndige Pipeline auf komplexem Bild testen
- **Bild:** `training_data/complex_pids/page_1_original.png` (Komplexes Bild)
- **Ground Truth:** `training_data/complex_pids/page_1_original_truth_cgm.json`
- **Aktiviert:** Phase 2 (Swarm + Monolith), Phase 2c (Fusion), Phase 4 (Post-Processing)
- **Deaktiviert:** Predictive, Polyline, Self-Correction
- **Erwartung:** Beste F1-Scores durch Kombination von Swarm + Monolith + Fusion

### Test 5a: Test 4 + Predictive (2d)
- **Ziel:** Predictive Completion testen
- **Bild:** `training_data/complex_pids/page_1_original.png` (Komplexes Bild)
- **Ground Truth:** `training_data/complex_pids/page_1_original_truth_cgm.json`
- **Aktiviert:** Wie Test 4 + Phase 2d (Predictive)
- **Erwartung:** Verbesserte Recall durch Predictive Completion

### Test 5b: Test 4 + Polyline (2e)
- **Ziel:** Polyline Refinement testen
- **Bild:** `training_data/complex_pids/page_1_original.png` (Komplexes Bild)
- **Ground Truth:** `training_data/complex_pids/page_1_original_truth_cgm.json`
- **Aktiviert:** Wie Test 4 + Phase 2e (Polyline)
- **Erwartung:** Verbesserte Precision durch Polyline Refinement

### Test 5c: Test 4 + Self-Correction (3)
- **Ziel:** Self-Correction Loop testen
- **Bild:** `training_data/complex_pids/page_1_original.png` (Komplexes Bild)
- **Ground Truth:** `training_data/complex_pids/page_1_original_truth_cgm.json`
- **Aktiviert:** Wie Test 4 + Phase 3 (Self-Correction)
- **Erwartung:** Verbesserte F1-Scores durch iterative Korrektur

---

## ğŸ“ Output-Ordnerstruktur

Alle Tests speichern ihre Ergebnisse in einer sauberen Ordnerstruktur:

```
outputs/strategy_validation/
â”œâ”€â”€ validation_YYYYMMDD_HHMMSS.json          # Validierungsergebnisse
â”œâ”€â”€ summary_YYYYMMDD_HHMMSS.json            # Finale Zusammenfassung aller Tests
â”‚
â”œâ”€â”€ Test_1_Baseline_Phase_1_(Legenden-Erkennung)/
â”‚   â”œâ”€â”€ pipeline.log                        # Pipeline-Logs (alle Phasen)
â”‚   â”œâ”€â”€ logs/                               # LLM-Logs
â”‚   â”‚   â””â”€â”€ llm_calls_YYYYMMDD_HHMMSS.log
â”‚   â”œâ”€â”€ results.json                        # VollstÃ¤ndige Analyse-Ergebnisse
â”‚   â”œâ”€â”€ kpis.json                           # KPIs (falls Ground Truth verfÃ¼gbar)
â”‚   â””â”€â”€ [weitere Pipeline-Outputs]          # Debug-Maps, Visualisierungen, etc.
â”‚
â”œâ”€â”€ Test_2_Baseline_Simple_PID_(Monolith-All)/
â”‚   â”œâ”€â”€ pipeline.log
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ results.json
â”‚   â”œâ”€â”€ kpis.json
â”‚   â””â”€â”€ [weitere Pipeline-Outputs]
â”‚
â”œâ”€â”€ Test_3_Baseline_Swarm-Only/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ Test_4_Baseline_Complex_PID_(Spezialisten-Kette)/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ Test_5a_Test_4_+_Predictive_(2d)/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ Test_5b_Test_4_+_Polyline_(2e)/
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ Test_5c_Test_4_+_Self-Correction_(3)/
    â””â”€â”€ ...
```

### Dateien pro Test-Ordner

1. **`pipeline.log`** - Alle Pipeline-Logs (Phase 0-4)
2. **`logs/llm_calls_*.log`** - Alle LLM-Aufrufe (Requests/Responses)
3. **`results.json`** - VollstÃ¤ndige Analyse-Ergebnisse (Elements, Connections)
4. **`kpis.json`** - KPIs (F1, Precision, Recall) wenn Ground Truth verfÃ¼gbar
5. **Weitere Pipeline-Outputs:**
   - `*_debug_map.png` - Debug-Visualisierungen
   - `*_confidence_map.png` - Confidence-Maps
   - `*_score_curve.png` - Score-Kurven
   - `*_kpi_dashboard.png` - KPI-Dashboards
   - `*_report.html` - HTML-Reports
   - `output_phase_*.json` - Zwischenergebnisse pro Phase

---

## ğŸš€ Test-AusfÃ¼hrung

### Voraussetzungen

1. **GCP-Credentials setzen:**
   ```powershell
   $env:GCP_PROJECT_ID='dein_project_id'
   $env:GCP_LOCATION='us-central1'
   ```

2. **Test-Konfiguration validieren:**
   - Das Skript validiert automatisch alle Bilder und Ground Truth-Dateien
   - Fehler werden vor Test-Start gemeldet

### Einzelnen Test ausfÃ¼hren

```bash
# Test 2 (empfohlen zum Start)
python scripts/validation/run_strategy_validation.py --test "Test 2"

# Test 4 (vollstÃ¤ndige Pipeline)
python scripts/validation/run_strategy_validation.py --test "Test 4"

# Test 5c (mit Self-Correction)
python scripts/validation/run_strategy_validation.py --test "Test 5c"
```

### Alle Tests ausfÃ¼hren

```bash
python scripts/validation/run_strategy_validation.py --test all
```

**Laufzeit:** ~20-30 Minuten fÃ¼r alle 7 Tests

---

## ğŸ“Š Validierung

### Automatische Validierung

Das Skript validiert automatisch:

1. **Bilder:** Existieren alle Test-Bilder?
2. **Ground Truth:** Existieren alle Ground Truth-Dateien?
3. **JSON-Struktur:** Sind Ground Truth-Dateien gÃ¼ltig?
4. **Elemente/Verbindungen:** Wie viele Elemente/Verbindungen in Ground Truth?

**Validierungsergebnisse werden gespeichert:**
- `outputs/strategy_validation/validation_YYYYMMDD_HHMMSS.json`

### Manuelle Validierung

Nach Test-AusfÃ¼hrung:

1. **Ergebnisse prÃ¼fen:** `outputs/strategy_validation/Test_X_*/results.json`
2. **KPIs prÃ¼fen:** `outputs/strategy_validation/Test_X_*/kpis.json`
3. **Logs prÃ¼fen:** `outputs/strategy_validation/Test_X_*/pipeline.log`
4. **Zusammenfassung prÃ¼fen:** `outputs/strategy_validation/summary_*.json`

---

## ğŸ“ˆ Datenanalyse

### Nach Test-AusfÃ¼hrung

Alle Daten sind in `outputs/strategy_validation/` gespeichert:

1. **Zusammenfassung:** `summary_YYYYMMDD_HHMMSS.json`
   - EnthÃ¤lt alle KPIs aller Tests
   - ErmÃ¶glicht Vergleich zwischen Tests

2. **Einzelne Test-Ergebnisse:** `Test_X_*/kpis.json`
   - Detaillierte KPIs pro Test
   - VollstÃ¤ndige KPI-Struktur

3. **Logs:** `Test_X_*/pipeline.log` und `Test_X_*/logs/`
   - VollstÃ¤ndige Pipeline-Logs
   - LLM-Aufrufe fÃ¼r Debugging

### Datenanalyse-Skripte

```python
# Beispiel: Zusammenfassung laden
import json
from pathlib import Path

summary_file = Path("outputs/strategy_validation/summary_*.json")
with open(summary_file, 'r') as f:
    summary = json.load(f)

# KPIs vergleichen
for test_name, kpis in summary['results'].items():
    print(f"{test_name}:")
    print(f"  Element F1: {kpis.get('element_f1', 0.0):.4f}")
    print(f"  Connection F1: {kpis.get('connection_f1', 0.0):.4f}")
```

---

## ğŸ”§ Pipeline-Abstimmung

### Nach Test-AusfÃ¼hrung

1. **Ergebnisse analysieren:**
   - Welche Tests haben die besten F1-Scores?
   - Welche Komponenten verbessern die Performance?
   - Welche Komponenten verschlechtern die Performance?

2. **Parameter anpassen:**
   - IoU-Thresholds
   - Confidence-Thresholds
   - Self-Correction-Parameter

3. **Erneut testen:**
   - Tests mit angepassten Parametern ausfÃ¼hren
   - Ergebnisse vergleichen

4. **Iterativ verbessern:**
   - Test â†’ Analyse â†’ Anpassung â†’ Test
   - Bis optimale Performance erreicht ist

---

## âœ… Checkliste

### Vor Test-AusfÃ¼hrung

- [ ] GCP-Credentials gesetzt
- [ ] Test-Bilder vorhanden
- [ ] Ground Truth-Dateien vorhanden
- [ ] Output-Verzeichnis erstellt (`outputs/strategy_validation/`)

### Nach Test-AusfÃ¼hrung

- [ ] Alle Tests erfolgreich abgeschlossen
- [ ] Ergebnisse in `outputs/strategy_validation/` gespeichert
- [ ] Logs verfÃ¼gbar
- [ ] KPIs berechnet (wenn Ground Truth verfÃ¼gbar)
- [ ] Zusammenfassung erstellt

---

## ğŸ¯ NÃ¤chste Schritte

1. **Tests ausfÃ¼hren:** Starte mit Test 2 (einfachster Test)
2. **Ergebnisse analysieren:** PrÃ¼fe KPIs und Logs
3. **Pipeline abstimmen:** Passe Parameter basierend auf Ergebnissen an
4. **Erneut testen:** Validiere Verbesserungen
5. **Finale Pipeline:** Optimale Konfiguration festlegen

---

**Status:** âœ… **Bereit fÃ¼r Test-AusfÃ¼hrung**

